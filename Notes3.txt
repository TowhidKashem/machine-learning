1. Traditional Machine Learning
  - is more manual
  - the engineer does feature extraction and tries to find patterns in the data before feeding it into the model
  - then the model does the classification or regression work
  - example lib: scikit learn


2. Deep Learning
  - the machine does both feature extraction and regression/classification work
  - example libs: tensorflow / keras

------

Model
  - simply a function that the computer writes (instead of the engineer) to do some work
  - to come up with the model we give the computer some input and labeled output (to check it's answers)


2 types of models/problems:
  - classification e.g. detect the dog breed
  - regression e.g. predict housing prices for the next year


sample = row in a table

hyper parameters
  - little knobs on the model that can be tweaked and dialed to get better results
  - this process is known as parameter optimization

------

Libraries:

1. Tensorflow
  - low level AI framework developed by Google
  - can be used for many things but usually used for traning deep nueral networks
  - a tensor is a multidimensional array that holds an immutable value (like numbers)
  - used for high performance models working on large datasets, difficult to debug
  - hard to develop in directly, usually only used on it's own in research work


2. Keras
  - is a higher level nueral network framework/API
  - is a wrapper for Tensorflow
  - can be used to quickly build and deploy deep NN
  - used for smaller models working on smaller datasets, easy to debug
  - high level of abstraction makes it easy to develop in for new comers


3. Pytorch
  - low level API developed by Facebook
  - for natural language processing and computer vision
  - is a more powerful version of NumPy
  - used for high performance models working on large datasets, easier to debug than both keras and tensorflow
  - easier to develop in than tensorflow but harder than keras


